{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1 Get Started on spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2 Linguistic Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process text/corpus with nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Dostoevsky was the son of a doctor. \n",
    "His parents were very hard-working and deeply religious people,\n",
    "but so poor that they lived with their five children in only\n",
    "two rooms. The father and mother spent their evenings\n",
    "in reading aloud to their children, generally from books of\n",
    "a serious character.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get text from file\n",
    "\n",
    "text = open('sample.txt').read()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('research_paper.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INFO_text = [text for text in df[df['Conference'] == 'INFOCOM']['Title']]\n",
    "INFO_text\n",
    "doc = nlp(INFO_text[0])\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u'Today is a great day')\n",
    "[word.text for word in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = open('sample.txt').read()\n",
    "# doc = nlp(text)\n",
    "\n",
    "word_tokens = [token.text for token in doc]\n",
    "word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tally occurrences of words in a list\n",
    "from collections import Counter\n",
    "\n",
    "cnt = Counter()\n",
    "for word in ['red', 'blue', 'red', 'green', 'blue', 'blue']:\n",
    "    cnt[word] += 1\n",
    "cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ex: Count the Unique Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('sample.txt').read()\n",
    "doc = nlp(text)\n",
    "\n",
    "word_tokens = [token.text for token in doc]\n",
    "\n",
    "for word in word_tokens:\n",
    "    cnt[word] += 1\n",
    "cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text= \"Clutching the coin, Maria ran to the shops. She went straight to the counter and bought the sweets\"\n",
    "words = re.findall(r'\\w+',text)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "words = re.findall(r'\\w+', open('sample.txt').read().lower())\n",
    "# Counter(words)\n",
    "Counter(words).most_common(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Tokenization\n",
    "\n",
    "doc = nlp(u\"This is a sentence. This is another sentence.\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('sample.txt').read()\n",
    "doc = nlp(text)\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc[1].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[word.is_stop for word in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab[\"the\"].is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[word.is_stop for word in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default stop words in spaCy\n",
    "\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default stop words in NLTK\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the punctuations\n",
    "\n",
    "import string\n",
    "punctuations = string.punctuation\n",
    "punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ex: Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text= \"Clutching the coin, Maria ran to the shops. She went straight to the counter and bought the sweets\"\n",
    "doc = nlp(text)\n",
    "\n",
    "tokens = [tok.text for tok in doc if tok.text not in stopwords and tok.text not in punctuations]\n",
    "cleaned_doc = ' '.join(tokens)\n",
    "cleaned_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "# stemmer = LancasterStemmer()\n",
    "\n",
    "text = \"I ran to the clinic with running nose\"\n",
    "words = word_tokenize(text)\n",
    "words\n",
    "[stemmer.stem(w) for w in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = nlp(u'running run')\n",
    "doc = nlp(u'meaning mean')\n",
    "doc = nlp(u'meanness meaning mean')\n",
    "\n",
    "for token in doc:\n",
    "\tprint(token.text,token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u\"Apples and oranges are similar. Boots and hippos aren't.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = open('sample.txt').read()\n",
    "# doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    print(token, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemma for Pronoun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text= \"She He They We\"\n",
    "doc = nlp(text)\n",
    "[tok.lemma_ for tok in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ex: Remove Pronoun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text= \"Clutching the coin, Maria ran to the shops. She went straight to the counter and bought the sweets\"\n",
    "doc = nlp(text)\n",
    "tokens = [tok.text for tok in doc]\n",
    "print(tokens)\n",
    "tokens = [tok.lemma_ for tok in doc if tok.lemma_ != '-PRON-']\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [tok for tok in tokens if tok not in stopwords and tok not in punctuations]\n",
    "cleaned_doc = ' '.join(tokens)\n",
    "cleaned_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech (POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = open('sample.txt').read()\n",
    "# doc = nlp(text)\n",
    "\n",
    "doc = nlp(u'The cat sit on the mat')\n",
    "for token in doc:\n",
    "    print(token.text,token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for token in doc:\n",
    "    print(token.text,token.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noun Chuck and Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = open('sample.txt').read()\n",
    "# doc = nlp(text)\n",
    "\n",
    "doc = nlp(u'The cat sit on the mat')\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_, chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u\"Autonomous cars shift insurance liability toward manufacturers\")\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_,chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('Wall Street Journal just published an interesting piece on crypto currencies')\n",
    " \n",
    "for token in doc:\n",
    "    print(\"{0}/{1} <--{2}-- {3}/{4}\".format(\n",
    "        token.text, token.tag_, token.dep_, token.head.text, token.head.tag_))\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Visualize Dependency and POS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "doc = nlp(u'The cat sit on the mat')\n",
    "displacy.serve(doc, style='dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "doc = nlp(u'The cat sit on the mat')\n",
    "options = {'compact': True, 'bg': '#09a3d5',\n",
    "           'color': 'white', 'font': 'Source Sans Pro'}\n",
    "displacy.serve(doc, style='dep', options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "doc = nlp(u'The cat sit on the mat')\n",
    "html = displacy.render([doc], style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u'Lee Kuan Yew is the prime minister for Singapore')\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "doc = nlp(u'Lee Kuan Yew is the prime minister for Singapore')\n",
    "\n",
    "doc.user_data['title'] = 'This is a title'\n",
    "displacy.serve(doc, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "doc = nlp(u'Lee Kuan Yew is the prime minister for Singapore')\n",
    "\n",
    "colors = {'ORG': 'linear-gradient(90deg, #aa9cfc, #fc9ce7)'}\n",
    "options = {'ents': ['ORG'], 'colors': colors}\n",
    "displacy.serve(doc, style='ent', options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "doc = nlp(u'Lee Kuan Yew is the prime minister for Singapore')\n",
    "\n",
    "html = displacy.render([doc], style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "doc = nlp(u'Lee Kuan Yew is the prime minister for Singapore')\n",
    "\n",
    "colors = {'ORG': 'linear-gradient(90deg, #aa9cfc, #fc9ce7)'}\n",
    "options = {'ents': ['ORG'], 'colors': colors}\n",
    "html = displacy.render([doc], style='ent', jupyter=True,options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3 Processing Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disable Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en', disable=['parser', 'tagger'])\n",
    "doc = nlp(u'The cat sit on the mat')\n",
    "for token in doc:\n",
    "    print(token.text,token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "doc = nlp(u'The cat sit on the mat')\n",
    "for token in doc:\n",
    "    print(token.text,token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "nlp.remove_pipe('tagger')\n",
    "doc = nlp(u'The cat sit on the mat')\n",
    "for token in doc:\n",
    "    print(token.text,token.pos_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "nlp.rename_pipe('ner', 'entityrecognizer')\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Custom Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_component(doc):\n",
    "    print(\"After tokenization, this doc has %s tokens.\" % len(doc))\n",
    "    if len(doc) < 10:\n",
    "        print(\"This is a pretty short document.\")\n",
    "    return doc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "nlp.add_pipe(my_component, name='print_info', first=True)\n",
    "nlp.pipe_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u'The cat sit on the mat')\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex: Add Component to Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_component2(doc):\n",
    "    \n",
    "    for tok in doc:\n",
    "        print(tok.text, tok.pos_,tok.tag_)\n",
    "    return doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(my_component2, name=\"print_pos\",last=True)\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "nlp.add_pipe(my_component2, name=\"print_pos\", after='parser')\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex2 : Add A Clean Up Component to Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def clean_text(doc):\n",
    "    stopwords = nlp.Defaults.stop_words\n",
    "    punctuations = string.punctuation\n",
    "    doc = [tok.text for tok in doc if tok.text not in stopwords and tok.text not in punctuations and tok.text != '\\n']\n",
    "    doc = [tok.lower() for tok in doc]\n",
    "    doc = ' '.join(doc)\n",
    "    return nlp.make_doc(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4 Vectors & Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectroization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "data = pd.read_csv('research_paper.csv')[0:5]\n",
    "     \n",
    "count_vect = CountVectorizer(analyzer=clean_text)\n",
    "# count_vect = CountVectorizer()\n",
    "X_counts = count_vect.fit_transform(data['Title'])\n",
    "print(X_counts.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_counts_df = pd.DataFrame(X_counts.toarray())\n",
    "X_counts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "embedding = Word2Vec(gutenberg.sents(),min_count=1, window=5, size=32)\n",
    "\n",
    "print(embedding.most_similar('man', topn=5))\n",
    "print(embedding.most_similar('woman', topn=5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')\n",
    "tokens = nlp(u'dog cat banana afskfsd')\n",
    "\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trainred Word Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.0176e-01,  3.7057e-01,  2.1281e-02, -3.4125e-01,  4.9538e-02,\n",
       "        2.9440e-01, -1.7376e-01, -2.7982e-01,  6.7622e-02,  2.1693e+00,\n",
       "       -6.2691e-01,  2.9106e-01, -6.7270e-01,  2.3319e-01, -3.4264e-01,\n",
       "        1.8311e-01,  5.0226e-01,  1.0689e+00,  1.4698e-01, -4.5230e-01,\n",
       "       -4.1827e-01, -1.5967e-01,  2.6748e-01, -4.8867e-01,  3.6462e-01,\n",
       "       -4.3403e-02, -2.4474e-01, -4.1752e-01,  8.9088e-02, -2.5552e-01,\n",
       "       -5.5695e-01,  1.2243e-01, -8.3526e-02,  5.5095e-01,  3.6410e-01,\n",
       "        1.5361e-01,  5.5738e-01, -9.0702e-01, -4.9098e-02,  3.8580e-01,\n",
       "        3.8000e-01,  1.4425e-01, -2.7221e-01, -3.7016e-01, -1.2904e-01,\n",
       "       -1.5085e-01, -3.8076e-01,  4.9583e-02,  1.2755e-01, -8.2788e-02,\n",
       "        1.4339e-01,  3.2537e-01,  2.7226e-01,  4.3632e-01, -3.1769e-01,\n",
       "        7.9405e-01,  2.6529e-01,  1.0135e-01, -3.3279e-01,  4.3117e-01,\n",
       "        1.6687e-01,  1.0729e-01,  8.9418e-02,  2.8635e-01,  4.0117e-01,\n",
       "       -3.9222e-01,  4.5217e-01,  1.3521e-01, -2.8878e-01, -2.2819e-02,\n",
       "       -3.4975e-01, -2.2996e-01,  2.0224e-01, -2.1177e-01,  2.7184e-01,\n",
       "        9.1703e-02, -2.0610e-01, -6.5758e-01,  1.8949e-01, -2.6756e-01,\n",
       "        9.2639e-02,  4.3316e-01, -4.8868e-01, -3.8309e-01, -2.1910e-01,\n",
       "       -4.4183e-01,  9.8044e-01,  6.7423e-01,  8.4003e-01, -1.8169e-01,\n",
       "        1.7385e-01,  4.1848e-01,  1.6098e-01, -1.0490e-01, -4.1965e-01,\n",
       "       -3.5660e-01, -1.6837e-01, -6.3458e-01,  3.8422e-01, -3.5043e-01,\n",
       "        1.7486e-01,  5.3528e-01,  2.0143e-01,  3.7877e-02,  4.7105e-01,\n",
       "       -4.4344e-01,  1.6840e-01, -1.6685e-01, -2.4022e-01, -1.0077e-01,\n",
       "        3.0334e-01,  4.2730e-01,  3.3803e-01, -4.3481e-01,  1.1343e-01,\n",
       "        6.1958e-02,  6.1808e-02, -1.4007e-01,  8.2018e-02, -3.9130e-02,\n",
       "        5.1442e-02,  2.8725e-01,  5.8025e-01, -5.7641e-01, -3.4652e-01,\n",
       "        1.0132e-01,  1.4463e-01,  1.1569e-02, -3.3701e-01, -1.7586e-01,\n",
       "       -3.5724e-01, -2.1423e-01,  1.1429e-02,  4.7645e-01, -3.7463e-02,\n",
       "       -2.9488e-01, -1.7465e-01,  3.0255e-01,  6.0317e-01, -6.6790e-02,\n",
       "       -2.7050e+00, -7.0308e-01,  4.0548e-01,  6.2874e-01,  6.3080e-01,\n",
       "       -5.4513e-01, -9.6191e-03,  2.6533e-01,  2.3391e-01, -5.1886e-02,\n",
       "       -6.5759e-03,  1.8573e-02, -4.5693e-01, -7.0351e-02, -3.0621e-01,\n",
       "       -1.4018e-02, -2.0408e-01,  3.7100e-01, -3.2354e-01, -8.4646e-01,\n",
       "        2.7092e-01, -1.1961e-01, -9.5576e-02, -6.0464e-01,  4.2409e-02,\n",
       "        2.4656e-01,  3.8445e-02, -2.5467e-02, -9.2908e-02, -2.1356e-01,\n",
       "        3.6120e-01,  1.9113e-02,  6.2741e-02, -1.3083e-01, -1.5146e-03,\n",
       "        5.8238e-01, -1.8956e-01,  7.8105e-01,  1.0477e-02,  1.0928e+00,\n",
       "        1.0140e-01, -3.6248e-01, -1.1962e-01, -3.4462e-01, -5.5704e-01,\n",
       "        2.5797e-01,  3.3356e-01,  3.3194e-01, -3.1298e-01, -7.5547e-01,\n",
       "       -7.5290e-01, -9.3072e-02, -1.1173e-01, -5.7251e-01,  1.6639e-01,\n",
       "        6.3579e-01,  2.4006e-01, -2.9211e-01,  9.0182e-01,  1.2425e-01,\n",
       "       -5.7751e-01,  4.7986e-02, -4.2748e-01,  2.4446e-01,  4.7232e-02,\n",
       "        3.5694e-01,  4.4241e-01, -2.3055e-01,  6.6037e-01, -7.3983e-03,\n",
       "       -3.7857e-01,  2.2759e-01, -3.7138e-01,  3.1055e-01, -7.2105e-02,\n",
       "       -2.4490e-01, -3.9761e-02,  5.3650e-01, -4.1478e-01,  1.6563e-01,\n",
       "        3.3707e-01,  1.0920e-01,  3.7219e-01, -5.5727e-01, -7.8060e-01,\n",
       "        1.4251e-01, -3.5828e-01,  4.1638e-01,  2.1446e-01,  1.8410e-01,\n",
       "       -4.7704e-01, -2.2005e-02, -2.3634e-01, -2.2840e-01,  3.4722e-01,\n",
       "        2.3667e-01,  7.4249e-02, -8.8416e-02,  2.8618e-01, -4.6942e-01,\n",
       "       -4.3914e-01, -2.6474e-01, -3.0690e-01, -1.5260e-01, -8.4870e-02,\n",
       "        2.8410e-01, -1.8481e-01, -2.2122e-01, -1.1169e-01, -2.5241e-02,\n",
       "        4.5968e-02,  3.5343e-02,  2.2467e-01,  5.1556e-01, -6.5137e-04,\n",
       "        9.9559e-02, -1.4215e-01,  2.0136e-01,  2.8334e-01, -2.8772e-01,\n",
       "        3.7766e-02, -3.7608e-01, -1.1681e-01, -6.7020e-01, -4.6265e-02,\n",
       "        3.8784e-01, -3.2295e-02, -5.4291e-02, -4.5384e-01,  1.9552e-01,\n",
       "       -2.9470e-01,  8.5009e-01,  1.0345e-01,  9.7010e-02,  1.1339e-01,\n",
       "        3.9502e-01,  5.9043e-02,  2.1978e-01,  1.8845e-01, -1.5891e-01,\n",
       "       -1.0301e-01,  3.3164e-01,  6.1477e-02, -2.9848e-01,  4.4510e-01,\n",
       "        4.7329e-01,  2.6312e-01, -1.8495e-01,  1.4652e-01, -3.1510e-02,\n",
       "        2.2908e-02, -2.5929e-01, -3.0862e-01,  1.7545e-03, -1.8962e-01,\n",
       "        5.4789e-01,  3.1194e-01,  2.4693e-01,  2.9929e-01, -7.4861e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "dog = nlp.vocab['dog']\n",
    "dog.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    " \n",
    "cosine_similarity = lambda x, y: 1 - spatial.distance.cosine(x, y)\n",
    " \n",
    "man = nlp.vocab['man'].vector\n",
    "woman = nlp.vocab['woman'].vector\n",
    "queen = nlp.vocab['queen'].vector\n",
    "king = nlp.vocab['king'].vector\n",
    " \n",
    "# We now need to find the closest vector in the vocabulary to the result of \"man\" - \"woman\" + \"queen\"\n",
    "maybe_king = man - woman + queen\n",
    "computed_similarities = []\n",
    " \n",
    "for word in nlp.vocab:\n",
    "    # Ignore words without vectors\n",
    "    if not word.has_vector:\n",
    "        continue\n",
    " \n",
    "    similarity = cosine_similarity(maybe_king, word.vector)\n",
    "    computed_similarities.append((word, similarity))\n",
    " \n",
    "computed_similarities = sorted(computed_similarities, key=lambda item: -item[1])\n",
    "print([w[0].text for w in computed_similarities[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6618534 0.23552845\n",
      "0.67148364 0.2427285\n"
     ]
    }
   ],
   "source": [
    "banana = nlp.vocab['banana']\n",
    "dog = nlp.vocab['dog']\n",
    "fruit = nlp.vocab['fruit']\n",
    "animal = nlp.vocab['animal']\n",
    " \n",
    "print(dog.similarity(animal), dog.similarity(fruit)) \n",
    "print(banana.similarity(fruit), banana.similarity(animal)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8901765218466683\n",
      "0.9115828449161616\n",
      "0.7822956752876101\n"
     ]
    }
   ],
   "source": [
    "target = nlp(\"Cats are beautiful animals.\")\n",
    " \n",
    "doc1 = nlp(\"Dogs are awesome.\")\n",
    "doc2 = nlp(\"Some gorgeous creatures are felines.\")\n",
    "doc3 = nlp(\"Dolphins are swimming mammals.\")\n",
    " \n",
    "print(target.similarity(doc1)) \n",
    "print(target.similarity(doc2))  \n",
    "print(target.similarity(doc3))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 5 Machine Learning using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('research_paper.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data shape\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check no of unique conference titles\n",
    "df['Conference'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a quick check after split\n",
    "\n",
    "print('Research title sample:', train['Title'].iloc[0])\n",
    "print('Conference of this paper:', train['Conference'].iloc[0])\n",
    "print('Training Data Shape:', train.shape)\n",
    "print('Testing Data Shape:', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "sns.barplot(x = train['Conference'].unique(), y=train['Conference'].value_counts())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Define function to cleanup text by removing personal pronouns, stopwords, and puncuation\n",
    "def cleanup_text(docs, logging=False):\n",
    "    texts = []\n",
    "    counter = 1\n",
    "    for doc in docs:\n",
    "        if counter % 1000 == 0 and logging:\n",
    "            print(\"Processed %d out of %d documents.\" % (counter, len(docs)))\n",
    "        counter += 1\n",
    "        doc = nlp(doc, disable=['parser', 'ner'])\n",
    "        tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']\n",
    "        tokens = [tok for tok in tokens if tok not in stopwords and tok not in punctuations]\n",
    "        tokens = ' '.join(tokens)\n",
    "        texts.append(tokens)\n",
    "    return pd.Series(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "INFO_text = [text for text in train[train['Conference'] == 'INFOCOM']['Title']]\n",
    "\n",
    "IS_text = [text for text in train[train['Conference'] == 'ISCAS']['Title']]\n",
    "\n",
    "INFO_clean = cleanup_text(INFO_text)\n",
    "INFO_clean = ' '.join(INFO_clean).split()\n",
    "\n",
    "IS_clean = cleanup_text(IS_text)\n",
    "IS_clean = ' '.join(IS_clean).split()\n",
    "\n",
    "INFO_counts = Counter(INFO_clean)\n",
    "IS_counts = Counter(IS_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INFO_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INFO_common_words = [word[0] for word in INFO_counts.most_common(20)]\n",
    "INFO_common_counts = [word[1] for word in INFO_counts.most_common(20)]\n",
    "\n",
    "fig = plt.figure(figsize=(18,6))\n",
    "sns.barplot(x=INFO_common_words, y=INFO_common_counts)\n",
    "plt.title('Most Common Words used in the research papers for conference INFOCOM')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_common_words = [word[0] for word in IS_counts.most_common(20)]\n",
    "IS_common_counts = [word[1] for word in IS_counts.most_common(20)]\n",
    "\n",
    "fig = plt.figure(figsize=(18,6))\n",
    "sns.barplot(x=IS_common_words, y=IS_common_counts)\n",
    "plt.title('Most Common Words used in the research papers for conference ISCAS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "STOPLIST = set(stopwords.words('english') + list(ENGLISH_STOP_WORDS))\n",
    "SYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-\", \"...\", \"”\", \"”\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "import spacy\n",
    "spacy.load('en')\n",
    "from spacy.lang.en import English\n",
    "parser = English()\n",
    "\n",
    "class CleanTextTransformer(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        return [cleanText(text) for text in X]\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "    \n",
    "def cleanText(text):\n",
    "    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def tokenizeText(sample):\n",
    "    tokens = parser(sample)\n",
    "    lemmas = []\n",
    "    for tok in tokens:\n",
    "        lemmas.append(tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_)\n",
    "    tokens = lemmas\n",
    "    tokens = [tok for tok in tokens if tok not in STOPLIST]\n",
    "    tokens = [tok for tok in tokens if tok not in SYMBOLS]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=tokenizeText, ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clf = LinearSVC()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer), ('clf', clf)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "X_train = train['Title'].tolist()\n",
    "y_train = train['Conference'].tolist() \n",
    "\n",
    "# train\n",
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test['Title'].tolist()\n",
    "y_test = test['Conference'].tolist() \n",
    "# test\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"accuray = \",accuracy_score(y_test,y_pred))\n",
    "print(\"Top 10 features used to predict: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printNMostInformative(vectorizer, clf, N):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    topClass1 = coefs_with_fns[:N]\n",
    "    topClass2 = coefs_with_fns[:-(N + 1):-1]\n",
    "    print(\"Class 1 best: \")\n",
    "    for feat in topClass1:\n",
    "        print(feat)\n",
    "    print(\"Class 2 best: \")\n",
    "    for feat in topClass2:\n",
    "        print(feat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printNMostInformative(vectorizer, clf, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.classification_report(labelsTest1, preds,  target_names=df['Conference'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
